{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bc4e05e-724b-4e2d-85e6-85790d17ac8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"FinalCoursework\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88cb4c35-30d8-403e-b660-49a3be569bfa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Read the File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69d6f535-36a2-4414-8b11-ff1c13e13091",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dir_path=\"/mnt/data/project-data/2020tweets\"\n",
    "\n",
    "df_trump = spark.read.option(\"multiline\", True).csv(dir_path+\"/hashtag_donaldtrump.csv\", header=True, inferSchema=True)\n",
    "df_biden = spark.read.option(\"multiline\", True).csv(dir_path+\"/hashtag_joebiden.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a2dc94f-5768-47fe-9675-ef7835334335",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transer_data_type(data):\n",
    "  data = data.withColumn(\"likes\", data[\"likes\"].cast(\"int\"))\n",
    "  data = data.withColumn(\"retweet_count\", data[\"retweet_count\"].cast(\"int\"))\n",
    "  data = data.withColumn(\"user_followers_count\", data[\"user_followers_count\"].cast(\"int\"))\n",
    "  data = data.withColumn(\"lat\", data[\"lat\"].cast(\"float\"))\n",
    "  data = data.withColumn(\"long\", data[\"long\"].cast(\"float\"))\n",
    "  return data\n",
    "\n",
    "df_trump = transer_data_type(df_trump)\n",
    "df_biden = transer_data_type(df_biden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "305e3b79-b6c2-4899-91b2-52dad2d94e71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "remove_columns = ['created_at', 'tweet_id', 'user_id', 'user_screen_name', 'user_join_date', 'collected_at', 'user_name', 'user_description', 'user_location', 'lat', 'long', 'city', 'country', 'continent', 'state', 'state_code']\n",
    "df_trump = df_trump.drop(*remove_columns)\n",
    "df_biden = df_biden.drop(*remove_columns)\n",
    "\n",
    "#Dropping rows with na values\n",
    "df_trump = df_trump.na.drop()\n",
    "df_biden = df_biden.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c679f358-0334-4cd6-89bf-24702518d7d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Compared the number between biden and trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "877d291c-a2ec-454b-820c-c2cc77379f0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a bar plot\n",
    "#plt.bar(['Trump', 'Biden'], [df_trump.count(), df_biden.count()])\n",
    "#plt.xlabel('Candidate')\n",
    "#plt.ylabel('Number of Tweets')\n",
    "#plt.title('Number of Tweets for Trump and Biden')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef8c5b55-a749-4229-b027-8e5efeebc0b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "#Adding a presidents column where 0 represents trump and 1 represents biden. This will turn it into a classification problem\n",
    "df_trump = df_trump.withColumn('President', lit(0))\n",
    "df_biden = df_biden.withColumn('President', lit(1))\n",
    "\n",
    "#Merging the 2 datasets\n",
    "df = df_trump.union(df_biden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5ad88aa-5361-43c6-9c70-7771d9decc96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------+------------------+--------------------+---------+\n|               tweet|likes|retweet_count|            source|user_followers_count|President|\n+--------------------+-----+-------------+------------------+--------------------+---------+\n|#Elecciones2020 |...|    0|            0|         TweetDeck|                1860|        0|\n|Usa 2020, Trump c...|   26|            9|  Social Mediaset |             1067661|        0|\n|#Trump: As a stud...|    2|            1|   Twitter Web App|                1185|        0|\n|2 hours since las...|    0|            0|     Trumpytweeter|                  32|        0|\n|You get a tie! An...|    4|            3|Twitter for iPhone|                5393|        0|\n+--------------------+-----+-------------+------------------+--------------------+---------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9210e3a7-e7d2-4610-93e7-5ef4ee63abbd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Caculate HashTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae25bf68-9338-4c1e-92e2-ce11e585f12a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "#Extracts all the hastags from the tweet and stores them in a list\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def getHashtagList(tweet):\n",
    "  tweet = tweet.lower()\n",
    "  tweet = re.sub(r'[^\\w\\s#]', '', tweet)\n",
    "  hashtag_list = []\n",
    "\n",
    "  for word in tweet.split():\n",
    "    if word[0] == '#':\n",
    "      hashtag_list.append(word)\n",
    "  \n",
    "  #Creates a dictionary where the hashtags are the key and their frequency in the list is the value\n",
    "  count_hashtags = Counter(hashtag_list)\n",
    "\n",
    "  #Remove duplicate hashtags\n",
    "  unique_hashtags = [hashtag for hashtag, frequency in count_hashtags.items() if frequency == 1]\n",
    "\n",
    "  return hashtag_list\n",
    "\n",
    "#Creates a column called hashtags which stores a hastag list for all the tweets\n",
    "df = df.withColumn('hashtags', getHashtagList(df['tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "428fc470-92a4-4b06-83b6-b3cbd7432a1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------+------------------+--------------------+---------+--------------------+\n|               tweet|likes|retweet_count|            source|user_followers_count|President|            hashtags|\n+--------------------+-----+-------------+------------------+--------------------+---------+--------------------+\n|#Elecciones2020 |...|    0|            0|         TweetDeck|                1860|        0|[#elecciones2020,...|\n|Usa 2020, Trump c...|   26|            9|  Social Mediaset |             1067661|        0|      [#donaldtrump]|\n|#Trump: As a stud...|    2|            1|   Twitter Web App|                1185|        0|            [#trump]|\n|2 hours since las...|    0|            0|     Trumpytweeter|                  32|        0|            [#trump]|\n|You get a tie! An...|    4|            3|Twitter for iPhone|                5393|        0|     [#trump, #iowa]|\n+--------------------+-----+-------------+------------------+--------------------+---------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "286963e6-893c-45b4-b626-e0c0a563979e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "#Splits the hashtag list and displays each hashtag in its own row in column hashtag\n",
    "df_exploded = df.withColumn('hashtag', F.explode(df['hashtags']))\n",
    "\n",
    "#Creates a new column called count which displays how many rows the hashtag appears in\n",
    "#The counts will be used in TF-IDF calculation\n",
    "df_count = df_exploded.groupBy(\"hashtag\").count()\n",
    "\n",
    "#Renaming count to hashtag_count\n",
    "df_count = df_count.withColumnRenamed('count', 'hashtag_count')\n",
    "\n",
    "#Joining df_count with df_exploded on hashtag\n",
    "df_exploded = df_exploded.join(df_count, on=\"hashtag\", how=\"left\")\n",
    "\n",
    "# Group by original columns and get the list of counts column\n",
    "df = df_exploded.groupBy(df.columns).agg(F.collect_list(\"hashtag_count\").alias(\"hashtag_counts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "941477fc-01f2-46ad-8911-3101661417da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------+-------------------+--------------------+---------+--------------------+--------------------+\n|               tweet|likes|retweet_count|             source|user_followers_count|President|            hashtags|      hashtag_counts|\n+--------------------+-----+-------------+-------------------+--------------------+---------+--------------------+--------------------+\n| He’s nothing mor...|    0|            0|    Twitter Web App|                  71|        0|            [#trump]|            [968674]|\n| it shows that #t...|    1|            0|Twitter for Android|                  11|        0|            [#trump]|            [968674]|\n| media-sourced po...|    0|            0|Twitter for Android|                 657|        0|            [#biden]|            [631862]|\n| mientras hace ca...|    0|            0| Twitter for iPhone|                  14|        1|[#electionday, #d...|[631862, 78352, 6...|\n| so Please get a ...|    0|            0|Twitter for Android|                 208|        0|             [#mask]|          [682, 682]|\n+--------------------+-----+-------------+-------------------+--------------------+---------+--------------------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d30b57a-856a-4bde-87f0-1accbe4aed64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1643761\n"
     ]
    }
   ],
   "source": [
    "#Total number of rows\n",
    "#This value is used to calculate TF-IDF\n",
    "df_count = df.count()\n",
    "print(df_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d932b419-478d-4763-8518-4e7f8b47982c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "import math\n",
    "\n",
    "#Creates the TF_IDF list column\n",
    "@udf(returnType=FloatType())\n",
    "def getAvg_TF_IDF(hashtags, hashtag_counts, df_count):\n",
    "  avg_TF_IDF = 0\n",
    "  count = 0\n",
    "\n",
    "  #If there are no hashtags the tweet will be given a value of 0\n",
    "  if len(hashtags) == 0:\n",
    "    return 0.0\n",
    "\n",
    "  for hashtag in hashtags:\n",
    "    #Calculating TF\n",
    "    #Since all the hastags are unique 1 is being divided by the total number of hastags\n",
    "    TF = 1 / len(hashtags)\n",
    "    #nDocs is the number of rows/tweets the hastag is used\n",
    "    nDocs = hashtag_counts[count]\n",
    "    #Calculating IDF\n",
    "    #df_count is the total number of rows/tweets\n",
    "    IDF = math.log(df_count / nDocs)\n",
    "    TF_IDF = TF * IDF\n",
    "    avg_TF_IDF += TF_IDF\n",
    "    count += 1\n",
    "\n",
    "  avg_TF_IDF /= count\n",
    "  return avg_TF_IDF\n",
    "\n",
    "# Apply the UDF to your DataFrame\n",
    "df = df.withColumn('TF_IDF', getAvg_TF_IDF(df['hashtags'], df['hashtag_counts'], lit(df_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2991b2db-df6a-4836-8b4e-87afa655cff7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------+-------------------+--------------------+---------+--------------------+--------------------+---------+\n|               tweet|likes|retweet_count|             source|user_followers_count|President|            hashtags|      hashtag_counts|   TF_IDF|\n+--------------------+-----+-------------+-------------------+--------------------+---------+--------------------+--------------------+---------+\n| He’s nothing mor...|    0|            0|    Twitter Web App|                  71|        0|            [#trump]|            [968674]|0.5288141|\n| it shows that #t...|    1|            0|Twitter for Android|                  11|        0|            [#trump]|            [968674]|0.5288141|\n| media-sourced po...|    0|            0|Twitter for Android|                 657|        0|            [#biden]|            [631862]|0.9560712|\n| mientras hace ca...|    0|            0| Twitter for iPhone|                  14|        1|[#electionday, #d...|[631862, 78352, 6...|0.7559104|\n| so Please get a ...|    0|            0|Twitter for Android|                 208|        0|             [#mask]|          [682, 682]| 7.787468|\n+--------------------+-----+-------------+-------------------+--------------------+---------+--------------------+--------------------+---------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55af88e6-2b25-43b0-a259-c52668cf0160",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We remove stop_words,special punction and url from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "473ed749-ccd4-485a-9189-727fbbc6a5f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re,string\n",
    "# This code defines a function named \"tokenize\" that tokenizes the input text.\n",
    "# It removes special characters, converts the text to lowercase, and removes stopwords and punctuation.\n",
    "# The resulting tokens are added to a list and returned.\n",
    "\n",
    "# This code defines a list of stopwords to be removed from the text.\n",
    "stopwords = [u'rt', u're', u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your',\n",
    "             u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers',\n",
    "             u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what',\n",
    "             u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were',\n",
    "             u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a',\n",
    "             u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by',\n",
    "             u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after',\n",
    "             u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under',\n",
    "             u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all',\n",
    "             u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not',\n",
    "             u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don',\n",
    "             u'should', u'now']\n",
    "def tokenize(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*(),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    \n",
    "    # Remove special characters\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Split text into tokens and filter out stopwords, single characters, and '``'\n",
    "    tokens = [word for word in text.split() if word not in stopwords and len(word) > 1 and word != '``']\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11ccc48c-5af9-4518-9a41-85ce61693424",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "caculate polarity with sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41c660fc-b343-4850-b829-cf3117e05c5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to /home/spark-\n[nltk_data]     ee539b33-28e5-4532-9667-19/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sc.broadcast(sia)\n",
    "\n",
    "def calculate_sentiment(text):\n",
    "    #sia = SentimentIntensityAnalyzer()\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)\n",
    "    sentiment_scores = sia.polarity_scores(text)['compound']\n",
    "\n",
    "    '''if(sentiment_scores>0):\n",
    "      sentiment_scores=1\n",
    "    elif(sentiment_scores<0):\n",
    "      sentiment_scores=-1\n",
    "    else:\n",
    "      sentiment_scores=0'''\n",
    "    return sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74a015b0-8766-41c0-8033-ef939c70c11f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define UDF to tokenize the tweet attribute\n",
    "@udf(returnType=FloatType())\n",
    "def tokenize_tweet(text):\n",
    "    # Write code here to tokenize the tweet using the desired method/function\n",
    "    tokens = tokenize(text)\n",
    "    sent=calculate_sentiment(tokens)\n",
    "    return sent\n",
    "\n",
    "# Add a new column 'Tokens' to the DataFrame 'df' by calling the 'tokenize_tweet' UDF on the 'tweet' column\n",
    "df=df.withColumn('polarity', tokenize_tweet(df['tweet']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "355a84b9-7d21-47c8-83d2-b89d22fb8845",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------+-------------------+--------------------+---------+--------------------+--------------------+---------+--------+\n|               tweet|likes|retweet_count|             source|user_followers_count|President|            hashtags|      hashtag_counts|   TF_IDF|polarity|\n+--------------------+-----+-------------+-------------------+--------------------+---------+--------------------+--------------------+---------+--------+\n| He’s nothing mor...|    0|            0|    Twitter Web App|                  71|        0|            [#trump]|            [968674]|0.5288141|  0.0772|\n| it shows that #t...|    1|            0|Twitter for Android|                  11|        0|            [#trump]|            [968674]|0.5288141|  -0.128|\n| media-sourced po...|    0|            0|Twitter for Android|                 657|        0|            [#biden]|            [631862]|0.9560712|  0.7579|\n| mientras hace ca...|    0|            0| Twitter for iPhone|                  14|        1|[#electionday, #d...|[631862, 78352, 6...|0.7559104|     0.0|\n| so Please get a ...|    0|            0|Twitter for Android|                 208|        0|             [#mask]|          [682, 682]| 7.787468|  0.5994|\n+--------------------+-----+-------------+-------------------+--------------------+---------+--------------------+--------------------+---------+--------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e1e19d2-a6b4-4a42-9cc3-32fca5dc0c7e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Labeled the cloassfication\n",
    "if polarity>0 sent=1\n",
    "if polarity=0 sent=0\n",
    "if polarity<0 sent=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7c54c3d-9480-43f2-b117-83c03be01d89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''from pyspark.sql.functions import when, col\n",
    "\n",
    "df = df.withColumn('sent', when(col('polarity') > 0, 1)\n",
    "                        .when(col('polarity') <= 0, -1)\n",
    "                        .otherwise(0))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d1fa71e-9954-4b5a-b330-055aeefdf358",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------+-------------------+--------------------+---------+--------------------+--------------------+---------+--------+----+\n|               tweet|likes|retweet_count|             source|user_followers_count|President|            hashtags|      hashtag_counts|   TF_IDF|polarity|sent|\n+--------------------+-----+-------------+-------------------+--------------------+---------+--------------------+--------------------+---------+--------+----+\n| He’s nothing mor...|    0|            0|    Twitter Web App|                  71|        0|            [#trump]|            [968674]|0.5288141|  0.0772|   1|\n| it shows that #t...|    1|            0|Twitter for Android|                  11|        0|            [#trump]|            [968674]|0.5288141|  -0.128|  -1|\n| media-sourced po...|    0|            0|Twitter for Android|                 657|        0|            [#biden]|            [631862]|0.9560712|  0.7579|   1|\n| mientras hace ca...|    0|            0| Twitter for iPhone|                  14|        1|[#electionday, #d...|[13888, 4145, 641...|0.7559104|     0.0|  -1|\n| so Please get a ...|    0|            0|Twitter for Android|                 208|        0|             [#mask]|          [682, 682]| 7.787468|  0.5994|   1|\n+--------------------+-----+-------------+-------------------+--------------------+---------+--------------------+--------------------+---------+--------+----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2eb82459-7914-4fab-83be-ef3fa512360c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To use word2Vec,we download the pre-trained word2Vec model and broatcast them to the worker computer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f3d316c-a1f3-4e02-b331-24eb10ec1d26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/context.py:117: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- word: string (nullable = true)\n |-- vector: array (nullable = true)\n |    |-- element: float (containsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "lookup = sqlContext.read.parquet(\"/FileStore/word2vecM_simple/data\").alias(\"lookup\")\n",
    "lookup.printSchema()\n",
    "lookup_bd = sc.broadcast(lookup.rdd.collectAsMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18b15a16-aab0-4b1f-a7bd-9d61ceca3b93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Change the return type of the function from ndarray to list\n",
    "def doc2vec(document):\n",
    "    doc_vec = [0.0] * 100\n",
    "    tot_words = 0\n",
    "    \n",
    "    for word in document:\n",
    "        try:\n",
    "            vec = lookup_bd.value.get(word)\n",
    "            if vec is not None:\n",
    "                doc_vec = [a + b + 1 for a, b in zip(doc_vec, vec)]\n",
    "                tot_words += 1\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if tot_words > 0:\n",
    "        doc_vec = [a / float(tot_words) for a in doc_vec]\n",
    "    \n",
    "    return doc_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15dd58f0-d0e6-4cb4-853e-6e4138a33f2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Define UDF to calculate the vector representation of the document by summing up the feature values of each word in the document\n",
    "@udf(returnType=VectorUDT())\n",
    "def doc_to_vec(tokens):\n",
    "    token = tokenize(tokens)\n",
    "    # Write code here to calculate the vector representation of the document using the desired method/function\n",
    "    vector = doc2vec(token)\n",
    "    return DenseVector(vector)\n",
    "  \n",
    "# Add a new column 'DocVec' to the DataFrame 'df' by calling the 'doc_to_vec' UDF on the 'Tokens' column\n",
    "df = df.withColumn('DocVec', doc_to_vec(df['tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "674babc4-b36b-439d-96b8-2b308f4c0453",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------+-------------------+--------------------+---------+--------------------+--------------------+---------+--------+--------------------+\n|               tweet|likes|retweet_count|             source|user_followers_count|President|            hashtags|      hashtag_counts|   TF_IDF|polarity|              DocVec|\n+--------------------+-----+-------------+-------------------+--------------------+---------+--------------------+--------------------+---------+--------+--------------------+\n| He’s nothing mor...|    0|            0|    Twitter Web App|                  71|        0|            [#trump]|            [968674]|0.5288141|  0.0772|[1.04750266564743...|\n| it shows that #t...|    1|            0|Twitter for Android|                  11|        0|            [#trump]|            [968674]|0.5288141|  -0.128|[0.94366044364869...|\n| media-sourced po...|    0|            0|Twitter for Android|                 657|        0|            [#biden]|            [631862]|0.9560712|  0.7579|[1.05222470499575...|\n| mientras hace ca...|    0|            0| Twitter for iPhone|                  14|        1|[#electionday, #d...|[631862, 78352, 6...|0.7559104|     0.0|[0.92850224425395...|\n| so Please get a ...|    0|            0|Twitter for Android|                 208|        0|             [#mask]|          [682, 682]| 7.787468|  0.5994|[1.03676919988356...|\n+--------------------+-----+-------------+-------------------+--------------------+---------+--------------------+--------------------+---------+--------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6283fc7-3b3b-4692-aa17-b2c0ff0dbf3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Build Train_Data and Split Test and train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ad042a1-d2b6-433f-b056-91442c774647",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "seed = 654321\n",
    "\n",
    "train_df, test_df = df.randomSplit([0.7, 0.3], seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7e0ebd1-d959-4182-9a21-1f7855bdeded",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[tweet: string, likes: int, retweet_count: int, source: string, user_followers_count: int, President: int, hashtags: array<string>, hashtag_counts: array<bigint>, TF_IDF: float, polarity: float, DocVec: vector]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6874e51f-b5cc-4ad6-8909-30616685ce25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[tweet: string, likes: int, retweet_count: int, source: string, user_followers_count: int, President: int, hashtags: array<string>, hashtag_counts: array<bigint>, TF_IDF: float, polarity: float, DocVec: vector]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e9285d2-20a1-4b28-9de9-1fc73593951c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493285\n"
     ]
    }
   ],
   "source": [
    "'''test_size = test_df.count()\n",
    "print(test_size)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3ee65a1-d4b3-4972-afe2-51be7eec8e60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['likes', 'retweet_count', 'user_followers_count', 'polarity', 'TF_IDF']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler,StringIndexer\n",
    "feature_columns = ['likes', 'retweet_count', 'user_followers_count','polarity','TF_IDF']\n",
    "#feature_columns=df.columns\n",
    "print(feature_columns)\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf286493-5adb-4a03-af65-8a74ad9e8cf4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.614883890651449"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Create the RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"President\")\n",
    "\n",
    "# Create the pipeline with the VectorAssembler and RandomForestClassifier\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# Evaluate the accuracy of the predictions\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"President\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "coursework cwl",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
