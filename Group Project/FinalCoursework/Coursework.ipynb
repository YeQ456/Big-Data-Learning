{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "031c10f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68e4b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"FinalCoursework\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f061c593",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path=\"/mnt/data/project-data/2020tweets\"\n",
    "\n",
    "df_trump = spark.read.option(\"multiline\", True).csv(\"data/hashtag_donaldtrump.csv\", header=True, inferSchema=True)\n",
    "df_biden = spark.read.option(\"multiline\", True).csv(\"data/hashtag_joebiden.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28aab010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transer_data_type(data):\n",
    "  data = data.withColumn(\"likes\", data[\"likes\"].cast(\"int\"))\n",
    "  data = data.withColumn(\"retweet_count\", data[\"retweet_count\"].cast(\"int\"))\n",
    "  data = data.withColumn(\"user_followers_count\", data[\"user_followers_count\"].cast(\"int\"))\n",
    "  return data\n",
    "\n",
    "df_trump = transer_data_type(df_trump)\n",
    "df_biden = transer_data_type(df_biden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a1c0b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "remove_columns = ['created_at', 'tweet_id', 'user_id', 'user_screen_name', 'user_join_date', 'collected_at', 'user_name', 'user_description', 'user_location', 'lat', 'long', 'city', 'country', 'continent', 'state', 'state_code']\n",
    "df_trump = df_trump.drop(*remove_columns)\n",
    "df_biden = df_biden.drop(*remove_columns)\n",
    "\n",
    "#Dropping rows with na values\n",
    "df_trump = df_trump.na.drop()\n",
    "df_biden = df_biden.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b91858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a bar plot\n",
    "#plt.bar(['Trump', 'Biden'], [df_trump.count(), df_biden.count()])\n",
    "#plt.xlabel('Candidate')\n",
    "#plt.ylabel('Number of Tweets')\n",
    "#plt.title('Number of Tweets for Trump and Biden')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aff130ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "#Adding a presidents column where 0 represents trump and 1 represents biden. This will turn it into a classification problem\n",
    "df_trump = df_trump.withColumn('President', lit(0))\n",
    "df_biden = df_biden.withColumn('President', lit(1))\n",
    "\n",
    "#Merging the 2 datasets\n",
    "df = df_trump.union(df_biden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b347f4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f56edd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------+------------------+--------------------+---------+\n",
      "|               tweet|likes|retweet_count|            source|user_followers_count|President|\n",
      "+--------------------+-----+-------------+------------------+--------------------+---------+\n",
      "|#Elecciones2020 |...|    0|            0|         TweetDeck|                1860|        0|\n",
      "|Usa 2020, Trump c...|   26|            9|  Social Mediaset |             1067661|        0|\n",
      "|#Trump: As a stud...|    2|            1|   Twitter Web App|                1185|        0|\n",
      "|2 hours since las...|    0|            0|     Trumpytweeter|                  32|        0|\n",
      "|You get a tie! An...|    4|            3|Twitter for iPhone|                5393|        0|\n",
      "+--------------------+-----+-------------+------------------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "655d594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "#Extracts all the hastags from the tweet and stores them in a list\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def getHashtagList(tweet):\n",
    "  tweet = tweet.lower()\n",
    "  tweet = re.sub(r'[^\\w\\s#]', '', tweet)\n",
    "  hashtag_list = []\n",
    "\n",
    "  for word in tweet.split():\n",
    "    if word[0] == '#':\n",
    "      hashtag_list.append(word)\n",
    "  \n",
    "  #Creates a dictionary where the hashtags are the key and their frequency in the list is the value\n",
    "  count_hashtags = Counter(hashtag_list)\n",
    "\n",
    "  #Remove duplicate hashtags\n",
    "  unique_hashtags = [hashtag for hashtag, frequency in count_hashtags.items() if frequency == 1]\n",
    "\n",
    "  return hashtag_list\n",
    "\n",
    "#Creates a column called hashtags which stores a hastag list for all the tweets\n",
    "df = df.withColumn('hashtags', getHashtagList(df['tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5896c8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------+------------------+--------------------+---------+--------------------+\n",
      "|               tweet|likes|retweet_count|            source|user_followers_count|President|            hashtags|\n",
      "+--------------------+-----+-------------+------------------+--------------------+---------+--------------------+\n",
      "|#Elecciones2020 |...|    0|            0|         TweetDeck|                1860|        0|[#elecciones2020,...|\n",
      "|Usa 2020, Trump c...|   26|            9|  Social Mediaset |             1067661|        0|      [#donaldtrump]|\n",
      "|#Trump: As a stud...|    2|            1|   Twitter Web App|                1185|        0|            [#trump]|\n",
      "|2 hours since las...|    0|            0|     Trumpytweeter|                  32|        0|            [#trump]|\n",
      "|You get a tie! An...|    4|            3|Twitter for iPhone|                5393|        0|     [#trump, #iowa]|\n",
      "+--------------------+-----+-------------+------------------+--------------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e78b32cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "#Splits the hashtag list and displays each hashtag in its own row in column hashtag\n",
    "df_exploded = df.withColumn('hashtag', F.explode(df['hashtags']))\n",
    "\n",
    "#Creates a new column called count which displays how many rows the hashtag appears in\n",
    "#The counts will be used in TF-IDF calculation\n",
    "df_count = df_exploded.groupBy(\"hashtag\").count()\n",
    "\n",
    "#Renaming count to hashtag_count\n",
    "df_count = df_count.withColumnRenamed('count', 'hashtag_count')\n",
    "\n",
    "#Joining df_count with df_exploded on hashtag\n",
    "df_exploded = df_exploded.join(df_count, on=\"hashtag\", how=\"left\")\n",
    "\n",
    "# Group by original columns and collect list of counts\n",
    "df = df_exploded.groupBy(df.columns).agg(F.collect_list(\"hashtag_count\").alias(\"hashtag_counts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afe0564c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------+-------------------+--------------------+---------+--------------------+--------------------+\n",
      "|               tweet|likes|retweet_count|             source|user_followers_count|President|            hashtags|      hashtag_counts|\n",
      "+--------------------+-----+-------------+-------------------+--------------------+---------+--------------------+--------------------+\n",
      "| Heâ€™s nothing mor...|    0|            0|    Twitter Web App|                  71|        0|            [#trump]|            [968674]|\n",
      "| it shows that #t...|    1|            0|Twitter for Android|                  11|        0|            [#trump]|            [968674]|\n",
      "| media-sourced po...|    0|            0|Twitter for Android|                 657|        0|            [#biden]|            [631862]|\n",
      "| mientras hace ca...|    0|            0| Twitter for iPhone|                  14|        1|[#electionday, #d...|[631862, 78352, 6...|\n",
      "| so Please get a ...|    0|            0|Twitter for Android|                 208|        0|             [#mask]|          [682, 682]|\n",
      "+--------------------+-----+-------------+-------------------+--------------------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0748e796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1643761\n"
     ]
    }
   ],
   "source": [
    "#Total number of rows\n",
    "#This value is used to calculate TF-IDF\n",
    "df_count = df.count()\n",
    "print(df_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "511cd441",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "import math\n",
    "\n",
    "#Creates the TF_IDF list column\n",
    "@udf(returnType=FloatType())\n",
    "def getAvg_TF_IDF(hashtags, hashtag_counts, df_count):\n",
    "  avg_TF_IDF = 0\n",
    "  count = 0\n",
    "\n",
    "  #If there are no hashtags the tweet will be given a value of 0\n",
    "  if len(hashtags) == 0:\n",
    "    return 0.0\n",
    "\n",
    "  for hashtag in hashtags:\n",
    "    #Calculating TF\n",
    "    #Since all the hastags are unique 1 is being divided by the total number of hastags\n",
    "    TF = 1 / len(hashtags)\n",
    "    #nDocs is the number of rows/tweets the hastag is used\n",
    "    nDocs = hashtag_counts[count]\n",
    "    #Calculating IDF\n",
    "    #df_count is the total number of rows/tweets\n",
    "    IDF = math.log(df_count / nDocs)\n",
    "    TF_IDF = TF * IDF\n",
    "    avg_TF_IDF += TF_IDF\n",
    "    count += 1\n",
    "\n",
    "  avg_TF_IDF /= count\n",
    "  return avg_TF_IDF\n",
    "\n",
    "# Apply the UDF to your DataFrame\n",
    "df = df.withColumn('TF_IDF', getAvg_TF_IDF(df['hashtags'], df['hashtag_counts'], lit(df_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e3623ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------+-------------------+--------------------+---------+--------------------+--------------------+---------+\n",
      "|               tweet|likes|retweet_count|             source|user_followers_count|President|            hashtags|      hashtag_counts|   TF_IDF|\n",
      "+--------------------+-----+-------------+-------------------+--------------------+---------+--------------------+--------------------+---------+\n",
      "| Heâ€™s nothing mor...|    0|            0|    Twitter Web App|                  71|        0|            [#trump]|            [968674]|0.5288141|\n",
      "| it shows that #t...|    1|            0|Twitter for Android|                  11|        0|            [#trump]|            [968674]|0.5288141|\n",
      "| media-sourced po...|    0|            0|Twitter for Android|                 657|        0|            [#biden]|            [631862]|0.9560712|\n",
      "| mientras hace ca...|    0|            0| Twitter for iPhone|                  14|        1|[#electionday, #d...|[631862, 78352, 6...|0.7559104|\n",
      "| so Please get a ...|    0|            0|Twitter for Android|                 208|        0|             [#mask]|          [682, 682]| 7.787468|\n",
      "+--------------------+-----+-------------+-------------------+--------------------+---------+--------------------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d579c3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize NLTK resources\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#Define get functions\n",
    "def getTokenizedText(text):\n",
    "    return word_tokenize(text.lower())\n",
    "\n",
    "def getFilteredTokens(tokens):\n",
    "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "def getLemmatizedTokens(filtered_tokens):\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "#Define the UDF\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def tokenize_tweet(text):\n",
    "    # Tokenization\n",
    "    tokens = getTokenizedText(text)\n",
    "\n",
    "    # Remove stopwords and punctuation\n",
    "    filtered_tokens = getFilteredTokens(tokens)\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatized_tokens = getLemmatizedTokens(filtered_tokens)\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Apply the UDF to the DataFrame\n",
    "df = df.withColumn('tokens', tokenize_tweet(df['tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e6d4c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sc.broadcast(sia)\n",
    "\n",
    "def calculate_sentiment(text):\n",
    "    #sia = SentimentIntensityAnalyzer()\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)\n",
    "    sentiment_scores = sia.polarity_scores(text)['compound']\n",
    "\n",
    "    return sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8d34651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define UDF to tokenize the tweet attribute\n",
    "@udf(returnType=FloatType())\n",
    "def polarize_tweet(tokens):\n",
    "    sent=calculate_sentiment(tokens)\n",
    "    return sent\n",
    "\n",
    "# Add a new column 'Tokens' to the DataFrame 'df' by calling the 'tokenize_tweet' UDF on the 'tweet' column\n",
    "df=df.withColumn('polarity', polarize_tweet(df['tokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be272240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------+-------------------+--------------------+---------+--------------------+--------------------+---------+--------------------+--------+\n",
      "|               tweet|likes|retweet_count|             source|user_followers_count|President|            hashtags|      hashtag_counts|   TF_IDF|              tokens|polarity|\n",
      "+--------------------+-----+-------------+-------------------+--------------------+---------+--------------------+--------------------+---------+--------------------+--------+\n",
      "| Heâ€™s nothing mor...|    0|            0|    Twitter Web App|                  71|        0|            [#trump]|            [968674]|0.5288141|[nothing, predato...|  0.0772|\n",
      "| it shows that #t...|    1|            0|Twitter for Android|                  11|        0|            [#trump]|            [968674]|0.5288141|[show, trump, car...| -0.0772|\n",
      "| media-sourced po...|    0|            0|Twitter for Android|                 657|        0|            [#biden]|            [631862]|0.9560712|[polling, favor, ...|  0.7579|\n",
      "| mientras hace ca...|    0|            0| Twitter for iPhone|                  14|        1|[#electionday, #d...|[631862, 78352, 6...|0.7559104|[mientras, hace, ...|     0.0|\n",
      "| so Please get a ...|    0|            0|Twitter for Android|                 208|        0|             [#mask]|          [682, 682]| 7.787468|[please, get, mas...|  0.5994|\n",
      "+--------------------+-----+-------------+-------------------+--------------------+---------+--------------------+--------------------+---------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca1fd880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df = df.withColumn('sent', when(col('polarity') > 0, 1)\n",
    "                        .when(col('polarity') <= 0, -1)\n",
    "                        .otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b206d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------+-------------------+--------------------+---------+--------------------+--------------------+---------+--------------------+--------+----+\n",
      "|               tweet|likes|retweet_count|             source|user_followers_count|President|            hashtags|      hashtag_counts|   TF_IDF|              tokens|polarity|sent|\n",
      "+--------------------+-----+-------------+-------------------+--------------------+---------+--------------------+--------------------+---------+--------------------+--------+----+\n",
      "| Heâ€™s nothing mor...|    0|            0|    Twitter Web App|                  71|        0|            [#trump]|            [968674]|0.5288141|[nothing, predato...|  0.0772|   1|\n",
      "| it shows that #t...|    1|            0|Twitter for Android|                  11|        0|            [#trump]|            [968674]|0.5288141|[show, trump, car...| -0.0772|  -1|\n",
      "| media-sourced po...|    0|            0|Twitter for Android|                 657|        0|            [#biden]|            [631862]|0.9560712|[polling, favor, ...|  0.7579|   1|\n",
      "| mientras hace ca...|    0|            0| Twitter for iPhone|                  14|        1|[#electionday, #d...|[631862, 78352, 6...|0.7559104|[mientras, hace, ...|     0.0|  -1|\n",
      "| so Please get a ...|    0|            0|Twitter for Android|                 208|        0|             [#mask]|          [682, 682]| 7.787468|[please, get, mas...|  0.5994|   1|\n",
      "+--------------------+-----+-------------+-------------------+--------------------+---------+--------------------+--------------------+---------+--------------------+--------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b44d6aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 654321\n",
    "train_df, test_df = df.randomSplit([0.7, 0.3], seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54ca5972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[tweet: string, likes: int, retweet_count: int, source: string, user_followers_count: int, President: int, hashtags: array<string>, hashtag_counts: array<bigint>, TF_IDF: float, tokens: array<string>, polarity: float, sent: int]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29ec8c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[tweet: string, likes: int, retweet_count: int, source: string, user_followers_count: int, President: int, hashtags: array<string>, hashtag_counts: array<bigint>, TF_IDF: float, tokens: array<string>, polarity: float, sent: int]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef79d70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "feature_columns = ['likes', 'retweet_count', 'user_followers_count', 'sent','TF_IDF']\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50de4798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_assembler = assembler.transform(train_df)\n",
    "#df_assembler.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a85e514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Initialize the Decision Tree classifier\n",
    "dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"President\",maxDepth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dcc5d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Set up the pipeline\n",
    "pipeline = Pipeline(stages=[assembler, dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a73a891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c1604ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6370880930902014\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"President\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
