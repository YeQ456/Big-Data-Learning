{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "90c054d37f6d207b6c795082e660f097",
     "grade": false,
     "grade_id": "cell-e05969ec582a0b26",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Spark SQL\n",
    "## Lab assignment: Exercises with MapReduce on Spark SQL - Dataframes\n",
    "\n",
    "The aim of this notebook is to play with the DataFrame API of Apache Spark, aiming to solve the same MapReduce exercises we did for the previous labs.\n",
    "\n",
    "The key difference will be the programming style, which will be more SQL-like. You should always use the functions provided by SparkSQL, I will import these functions as 'F', so we know for sure we are using Spark functions and not Python functions!\n",
    "\n",
    "**Note**: I have decided to leave out the exercise about the list of common friends between pairs of friends with DataFrames. Whilst definitely possible, I don't see much point in doing so. You could try and let me know how you get on with it, but you might need to use User-Defined Functions, or other functions I didn't explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f9d8fa43bd5d45e9dd31f766244ea7a8",
     "grade": false,
     "grade_id": "cell-77734fdb789e84f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Submission and marking criteria\n",
    "\n",
    "You should complete this notebook and add your solutions to it. When you are done, rename your completed notebook as `ex03.ipynb`. \n",
    "\n",
    "Important notes:\n",
    "- The **group leader** must submit the `ex03.ipynb` file on Moodle.\n",
    "- **Each member of the group** must complete the peer review survey and their contribution statement using this [link](https://forms.office.com/e/aAFaCQ2uxU). **You can only submit this survey ONCE**.\n",
    "- This lab is marked out of 100 marks. We give you 10 marks for completing exercise 0 with our help, and the three remaining exercises are worth 30 marks each.\n",
    "- The marking will be focused on:\n",
    "    - Code that does solve the task correctly (15 marks).\n",
    "    - Efficiency of the solution (15 marks).\n",
    "    - Minor mistakes will deduct marks from each exercise.\n",
    "- **Submission deadline: 15th March 2024 at 3pm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9943afb7a5f822050995c1ba038ce4d",
     "grade": false,
     "grade_id": "cell-2cec9fd6229659a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cede54f3c493570a8b81ff2e2103faa4",
     "grade": false,
     "grade_id": "cell-40a6e2427a09ad72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The first thing we need to do to start working with Spark is to initialize the `SparkSession`. We will also import a few libraries we will use. *Remember if you are using Databricks that `spark` and `sc` are already available to you and don't need initializing.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Spark SQL Lab\") \\\n",
    "    .getOrCreate() \n",
    "\n",
    "sc = spark.sparkContext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F  # I import all Spark Functions, like length, when, explode...\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Helper function to test the correctness of the solutions\n",
    "def test(var, val, msg=\"\"):\n",
    "    print(\"1 test passed.\") if var == val else print(\"1 test failed. \" + msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5b54d35fcd453b8d0e605376a51d483",
     "grade": false,
     "grade_id": "cell-284cbe6140883e04",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 0: Word Count with DataFrames\n",
    "\n",
    "As a warm-up, let's implement the Word Count with DataFrames. As in the previous lab, you're as asked to implement a `word_count(file_path)` function that counts the number of words in a document or a number of text documents provided in the input path.\n",
    "\n",
    "**Input:** The path to a text file\n",
    "\n",
    "**Output:** (word, count) - only the 10 words with the highest frequency!\n",
    "\n",
    "Recommended steps:\n",
    "1. Read the file or files. Each line should be an element of the DataFrame.\n",
    "2. Split the lines into words. (*transformation*)\n",
    "3. Filter empty words (`''`) resulting from previous steps. (*transformation*)\n",
    "4. Count the number of occurrences of each word. (*action*)\n",
    "5. Return to the driver program the 10 most repeated words. (*action*)\n",
    "\n",
    "**Additional challenge**: Improve the word count, eliminating any punctuation marks and avoid word duplicated due to capitalisation. *Advice*: Use SQL functions like: regexp_replace, trim, col or lower.\n",
    "\n",
    "We are providing a step-by-step solution for this exercise below (yes! we are this nice!), but we suggest you try it yourself first.\n",
    "\n",
    "                                                                                                       [10 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5144d45aab8c1b1c7cc0c98431635141",
     "grade": false,
     "grade_id": "cell-75b6d31821a2b65a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col='the', count=20923),\n",
       " Row(col='and', count=16606),\n",
       " Row(col='to', count=13492),\n",
       " Row(col='of', count=12866),\n",
       " Row(col='that', count=7164),\n",
       " Row(col='a', count=7003),\n",
       " Row(col='in', count=6860),\n",
       " Row(col='I', count=5756),\n",
       " Row(col='he', count=5640),\n",
       " Row(col='for', count=4534)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def word_count(file_path):\n",
    "    # <FILL-IN WITH YOUR CODE>\n",
    "    lines_df = spark.read.text(file_path)\n",
    "    filtered = lines_df.filter(lines_df['value'] != '')\n",
    "    split_lines = filtered.select(F.split(filtered['value'], ' ').alias('words'))\n",
    "    word_df = split_lines.select(F.explode('words'))\n",
    "    word_count_df = word_df.groupBy('col').count()\n",
    "    ordered_word_count_df = word_count_df.orderBy('count', ascending = False)\n",
    "    output = ordered_word_count_df.limit(10)\n",
    "    return output.collect()\n",
    "    \n",
    "# Test the function with quixote.txt file\n",
    "word_count(\"data/quixote.txt\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e8a532106b3b344effb6eaebcd823835",
     "grade": false,
     "grade_id": "cell-7620a7a5daf517e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The program should pass the following test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f709c96a3dc96d23af1f7bc13f6890e8",
     "grade": true,
     "grade_id": "cell-ef044fd580a494e6",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(col='the', count=20923), Row(col='and', count=16606), Row(col='to', count=13492), Row(col='of', count=12866), Row(col='that', count=7164), Row(col='a', count=7003), Row(col='in', count=6860), Row(col='I', count=5756), Row(col='he', count=5640), Row(col='for', count=4534)]\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "top10_quixote = word_count(\"data/quixote.txt\")\n",
    "print(top10_quixote)\n",
    "test(top10_quixote, [('the', 20923), ('and', 16606), ('to', 13492), ('of', 12866), \n",
    "                                  ('that', 7164), ('a', 7003), ('in', 6860), ('I', 5756), ('he', 5640), \n",
    "                                  ('for', 4534)], \"Try again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-step Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = spark.read.text(\"data/quixote.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to play with this for a bit, so let's cache it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the content of this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------+\n",
      "|value                                                                           |\n",
      "+--------------------------------------------------------------------------------+\n",
      "|                                                                                |\n",
      "|The Project Gutenberg EBook of The History of Don Quixote by Miguel de Cervantes|\n",
      "|                                                                                |\n",
      "|This eBook is for the use of anyone anywhere at no cost and with                |\n",
      "|almost no restrictions whatsoever.  You may copy it, give it away or            |\n",
      "|re-use it under the terms of the Project Gutenberg License included             |\n",
      "|with this eBook or online at www.gutenberg.org                                  |\n",
      "|                                                                                |\n",
      "|                                                                                |\n",
      "|Title: The History of Don Quixote                                               |\n",
      "+--------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_text.select(\"*\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_text.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there isn't much structure in that DataFrame. Spark has inferred a single column, as a string, and every row is a line of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did before, let's try to split this by blank space. To do so, we can use the `split` function from SparkSQL. This function takes two arguments: the column name where you want to apply the split, and the 'pattern' you want to use to split the string upon. This function returns an array of Column type. Similar to Column operations, this function needs to be used in conjunction with a transformation (e.g. `select`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------+\n",
      "|split(value,  , -1)                                                                            |\n",
      "+-----------------------------------------------------------------------------------------------+\n",
      "|[]                                                                                             |\n",
      "|[The, Project, Gutenberg, EBook, of, The, History, of, Don, Quixote, by, Miguel, de, Cervantes]|\n",
      "|[]                                                                                             |\n",
      "|[This, eBook, is, for, the, use, of, anyone, anywhere, at, no, cost, and, with]                |\n",
      "|[almost, no, restrictions, whatsoever., , You, may, copy, it,, give, it, away, or]             |\n",
      "|[re-use, it, under, the, terms, of, the, Project, Gutenberg, License, included]                |\n",
      "|[with, this, eBook, or, online, at, www.gutenberg.org]                                         |\n",
      "|[]                                                                                             |\n",
      "|[]                                                                                             |\n",
      "|[Title:, The, History, of, Don, Quixote]                                                       |\n",
      "+-----------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_text.select(F.split(\"value\", \" \")) \\\n",
    "    .show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use `explode` to expand each one of the elements of the lists, so that, we create a Row for each element of the arrays in the column `value`. We will call the split function inside of the explode!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|      col|\n",
      "+---------+\n",
      "|         |\n",
      "|      The|\n",
      "|  Project|\n",
      "|Gutenberg|\n",
      "|    EBook|\n",
      "|       of|\n",
      "|      The|\n",
      "|  History|\n",
      "|       of|\n",
      "|      Don|\n",
      "+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_text.select( \\\n",
    "      F.explode(F.split(\"value\", \" \")) \\\n",
    "    ).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `alias()` to give a new name, e.g. `word`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|         |\n",
      "|      The|\n",
      "|  Project|\n",
      "|Gutenberg|\n",
      "|    EBook|\n",
      "|       of|\n",
      "|      The|\n",
      "|  History|\n",
      "|       of|\n",
      "|      Don|\n",
      "+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_text.select( \\\n",
    "        F.explode(F.split(\"value\", \" \")) \\\n",
    "        .alias(\"word\") \\\n",
    "    ).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter out empty words. That's very similar to what we did with RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      The|\n",
      "|  Project|\n",
      "|Gutenberg|\n",
      "|    EBook|\n",
      "|       of|\n",
      "|      The|\n",
      "|  History|\n",
      "|       of|\n",
      "|      Don|\n",
      "|  Quixote|\n",
      "+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_text.select( \\\n",
    "           F.explode(F.split(\"value\", \" \")) \\\n",
    "          .alias(\"word\")) \\\n",
    "      .filter(\"word != ''\") \\\n",
    "      .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With RDDs, we needed to transform this into a tuple k,value. But with DataFrames, we can do the same operation by telling Spark which one is the attribute that will be used to group the DataFrame (i.e. the only column: `word`).\n",
    "We can use `groupBy`, but this is a transformation that allows us to perform aggregations, it needs to be used together with an aggregation operation like count, max, avg, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string, count: bigint]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.select( \\\n",
    "              F.explode(F.split(\"value\", \" \")) \\\n",
    "              .alias(\"word\")) \\\n",
    "        .filter(\"word != ''\") \\\n",
    "        .groupBy(\"word\").count() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, we didn't get the word count, why? the method `count` as an aggregation is NOT an action! but a transformation. We still need to take it back to the driver program, by using `collect` or `show`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      word|count|\n",
      "+----------+-----+\n",
      "|    online|    4|\n",
      "|      July|    1|\n",
      "|    CASTLE|    8|\n",
      "|      XVII|    2|\n",
      "|      AWAY|    2|\n",
      "|      hope|   65|\n",
      "|     those|  652|\n",
      "|       few|   79|\n",
      "|Duffield’s|    1|\n",
      "|  everyday|    3|\n",
      "+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_text.select( \\\n",
    "              F.explode(F.split(\"value\", \" \")) \\\n",
    "              .alias(\"word\")) \\\n",
    "        .filter(\"word != ''\") \\\n",
    "        .groupBy(\"word\").count().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is not quite there yet, we wanted this to be in descending order. What we could do is to transform that aggregated DataFrame using `sort` (or `OrderBy`).  To use `sort`, we need to indicate the column in which we want to apply the operation (i.e. `count`), and we also want to do it in descending order. You could do this in different ways:\n",
    "\n",
    "`.sort(df.count.desc())`  or `sort(desc(\"count\"))`, or `sort(\"count\", ascending=False)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the|20923|\n",
      "| and|16606|\n",
      "|  to|13492|\n",
      "|  of|12866|\n",
      "|that| 7164|\n",
      "|   a| 7003|\n",
      "|  in| 6860|\n",
      "|   I| 5756|\n",
      "|  he| 5640|\n",
      "| for| 4534|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_text.select( \\\n",
    "              F.explode(F.split(\"value\", \" \")) \\\n",
    "              .alias(\"word\")) \\\n",
    "        .filter(\"word != ''\") \\\n",
    "        .groupBy(\"word\").count() \\\n",
    "        .sort(F.desc(\"count\")) \\\n",
    "        .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to unpersist your DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional challenge:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were to remove any punctuation marks, you need a function to correct all sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(column):\n",
    "    return F.lower(F.trim(F.regexp_replace(column, r'[^0-9a-zA-ZñÑáéíóúÁÉÍÓÚ ]+', ''))).alias('value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = spark.createDataFrame([(u'Hello!, how is it going?',), (u' removing underscore_!',), (u' *      Removing punctuation and blank spaces  * ,',)], \n",
    "                                 ['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply transform the DataFrame with that function applying the function to the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence.select(remove_punctuation(F.col(\"sentence\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7c15f07906bb1fd48a51d1ce7427b1fa",
     "grade": false,
     "grade_id": "cell-024c26d2f51c3d48",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Putting this into our word count function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9428c200908e5651b29090770e57f7e",
     "grade": true,
     "grade_id": "cell-d9afc18d9f1004e1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2378454240.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [21]\u001b[1;36m\u001b[0m\n\u001b[1;33m    enhanced_word_count(\"data/quixote.txt\")\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def enhanced_word_count(file_path):\n",
    "    # <FILL-IN WITH YOUR CODE>\n",
    "enhanced_word_count(\"data/quixote.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e5f4cfcbfbf94108fc6b671ab7c9a3d",
     "grade": false,
     "grade_id": "cell-cd1ed4f3427ea1cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 1. Histogram of word repetition\n",
    "\n",
    "Provide a histogram of word repetitions, that is, the number of words that are repeated X times:\n",
    "\n",
    "* 1 time - 3 words\n",
    "* 2 times - 10 words\n",
    "* 3 times, 20 words\n",
    "...\n",
    "\n",
    "You are asked to implement a `histogram_reps(file_path)` function in Spark that **must not** use the function `word_count(file_path)`, but it could use part of the code you did before. All the processing must be done with **DataFrames**, and there should be a single `collect()` at the end to return a list. The list must be ordered by the number of times.\n",
    "\n",
    "**Input**: The path to a text file\n",
    "\n",
    "**Output**: (number of repetitions, number of words)\n",
    "\n",
    "                                                                                                       [30 marks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96c253124c1be82aba643bfcb5bdac48",
     "grade": false,
     "grade_id": "cell-38476d16f40954cd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Frequency=1, count=17817),\n",
       " Row(Frequency=2, count=5146),\n",
       " Row(Frequency=3, count=2291),\n",
       " Row(Frequency=4, count=1520),\n",
       " Row(Frequency=5, count=998),\n",
       " Row(Frequency=6, count=737),\n",
       " Row(Frequency=7, count=589),\n",
       " Row(Frequency=8, count=439),\n",
       " Row(Frequency=9, count=333),\n",
       " Row(Frequency=10, count=288),\n",
       " Row(Frequency=11, count=227),\n",
       " Row(Frequency=12, count=216),\n",
       " Row(Frequency=13, count=184),\n",
       " Row(Frequency=14, count=199),\n",
       " Row(Frequency=15, count=143),\n",
       " Row(Frequency=16, count=128),\n",
       " Row(Frequency=17, count=109),\n",
       " Row(Frequency=18, count=97),\n",
       " Row(Frequency=19, count=91),\n",
       " Row(Frequency=20, count=90)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def histogram_reps(file_path):\n",
    "    # <FILL-IN WITH YOUR CODE>\n",
    "    lines_df = spark.read.text(file_path)\n",
    "    filtered = lines_df.filter(lines_df['value'] != '')\n",
    "    split_lines = filtered.select(F.split(filtered['value'], ' ').alias('words'))\n",
    "    word_df = split_lines.select(F.explode('words'))\n",
    "    word_count = word_df.groupBy('col').count()\n",
    "    word_frequency = word_count.select(word_count['count'].alias('Frequency'))\n",
    "    word_repetition = word_frequency.groupBy('Frequency').count()\n",
    "    output = word_repetition.orderBy('Frequency')\n",
    "    return output.collect()\n",
    "    \n",
    "histogram_reps('data/quixote.txt') [:20] # look at the first 20 results    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a544c7eef583dce482f2602922816325",
     "grade": true,
     "grade_id": "cell-bdde40e488c56163",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "hist_quixote = histogram_reps(\"data/quixote.txt\")\n",
    "test(hist_quixote[:10],[(1, 17817), (2, 5146), (3, 2291), (4, 1520), \n",
    "                                     (5, 998), (6, 737), (7, 589), (8, 439), (9, 333), (10, 288)], \"Try again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could plot this with the matplotlib library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_values, y_values) = zip(*hist_quixote[:10])\n",
    "plt.bar(x_values, y_values)\n",
    "plt.title('Histogram of repetitions (up to 10)')\n",
    "plt.xlabel('Number of repetitions')\n",
    "plt.ylabel('Number of words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dc16f0ec55f3c445756d2dfdb8ad91a4",
     "grade": false,
     "grade_id": "cell-5f7c2d07ff5e0889",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2. Histogram of the length of the words\n",
    "\n",
    "\n",
    "Provide a histogram of the length of the words. Word repetition is not a problem, so if you have the word 'bye' twice in your document, you would add 2 to the number of words of length 3.\n",
    "\n",
    "* Length 1 - 100 times\n",
    "* Length 2 - 300 times\n",
    "* Length 3 - 400 times\n",
    "...\n",
    "\n",
    "You are asked to implement a `histogram_length(file_path)` function in Spark. All the processing must be done with **DataFrames**, and there should be a single `collect()` at the end to return a list. The list must be ordered by the length of the words.\n",
    "\n",
    "**Input**: A text document or multiple text documents\n",
    "\n",
    "**Output**: (Length, number of words)\n",
    "\n",
    "**Note: We are going to assume that the maximum word length is 16 characters, so anything above that shouldn't appear in the result.**\n",
    "\n",
    "                                                                                                       [30 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "efee1ed48b1a2144673c4af16a59233b",
     "grade": false,
     "grade_id": "cell-7cccb15c1a1e97fb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def histogram_length(file_path):\n",
    "    # <FILL-IN WITH YOUR CODE>\n",
    "    lines_df = spark.read.text(file_path)\n",
    "    filtered = lines_df.filter(lines_df['value'] != '')\n",
    "    split_lines = filtered.select(F.split(filtered['value'], ' ').alias('words'))\n",
    "    word_df = split_lines.select(F.explode('words'))\n",
    "    filtered_word = word_df.filter(word_df['col'] != '')\n",
    "    word_length = filtered_word.select(F.length(word_df['col']).alias('Length'))\n",
    "    length_count = word_length.groupBy('Length').count()\n",
    "    output = length_count.orderBy('Length').limit(16)\n",
    "    return output.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48968bcb54f2e2fde4cf4371d8d68292",
     "grade": true,
     "grade_id": "cell-e460ec12b208f800",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Length=1, count=12978), Row(Length=2, count=80003), Row(Length=3, count=98414), Row(Length=4, count=80717), Row(Length=5, count=45809), Row(Length=6, count=33672), Row(Length=7, count=30995), Row(Length=8, count=19570), Row(Length=9, count=12350), Row(Length=10, count=7929), Row(Length=11, count=3849), Row(Length=12, count=2043), Row(Length=13, count=963), Row(Length=14, count=539), Row(Length=15, count=251), Row(Length=16, count=110)]\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "length_quixote = histogram_length(\"data/quixote.txt\")\n",
    "print(length_quixote)\n",
    "test(length_quixote, [(1, 12978), (2, 80003), (3, 98414), (4, 80717), (5, 45809), \n",
    "                                   (6, 33672), (7, 30995), (8, 19570), (9, 12350), (10, 7929),\n",
    "                                   (11, 3849), (12, 2043), (13, 963), (14, 539), (15, 251), (16, 110)],\"Try again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "93c0dc5f92724d7291a7b99be64e2e59",
     "grade": false,
     "grade_id": "cell-4246a9f0a617488f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 3. Average length of the words in a document\n",
    "\n",
    "You are now asked to implement an `average_length(file_path)` function in Spark that provides the average length of the words in a document or documents. All the processing must be done with  using **DataFrames**., and the last instruction must be the only one returning a result to the driver. \n",
    "\n",
    "**Input**: The path to a text file\n",
    "\n",
    "**Output**: Average length of the words\n",
    "\n",
    "**Note: Again, we are going to assume that the maximum word length is 16 characters, so anything above that shouldn't be used to compute the average**\n",
    "\n",
    "**Hint**: You might be able to use some built-in functions from Spark!\n",
    "\n",
    "                                                                                                       [30 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "42fa941a9167c743ab567552f0327b9a",
     "grade": false,
     "grade_id": "cell-c584893574d599e2",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.369345081077563"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def average_length(file_path):\n",
    "    # <FILL-IN WITH YOUR CODE>\n",
    "    lines_df = spark.read.text(file_path)\n",
    "    filtered = lines_df.filter(lines_df['value'] != '')\n",
    "    split_lines = filtered.select(F.split(filtered['value'], ' ').alias('words'))\n",
    "    word_df = split_lines.select(F.explode('words'))\n",
    "    filtered_word = word_df.filter(word_df['col'] != '')\n",
    "    word_length = filtered_word.select(F.length(word_df['col']).alias('Length'))\n",
    "    output = word_length.agg(F.avg('Length'))\n",
    "    return output.collect()[0][0]\n",
    "    \n",
    "average_length(\"data/quixote.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2274135a27bb41ba871722281a093978",
     "grade": false,
     "grade_id": "cell-d5d1b9609cbb789b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The program should pass the following test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b36a83b8a46e83c5c34a0467409b584",
     "grade": true,
     "grade_id": "cell-b707812456f2ad27",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "avg = average_length(\"data/quixote.txt\")\n",
    "test(round(avg,2), 4.37,'Try again!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
